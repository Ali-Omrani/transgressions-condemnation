{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603d448e",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fbfb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping jedi as it is not installed.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall jedi\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0be7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the jsonl dataset\n",
    "data_dir = Path(\"../joes_transgression_ambiguity_project/data/annotated_tweets/ta_tweets.jsonl\")\n",
    "\n",
    "import pandas as pd    \n",
    "jsonObj = pd.read_json(path_or_buf=data_dir, lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "021f7abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reject'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj.head()[\"answer\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3779822d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'meta', '_input_hash', '_task_hash', 'label', 'score',\n",
       "       'priority', 'spans', '_session_id', '_view_id', 'answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849720e",
   "metadata": {},
   "source": [
    "### Huggingface dataset loading script\n",
    "can be used later to publish the dataset on huggingface hub for better outreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a3772d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "This is script loads the annotations for the moral transgression - twitter condemnation project.\n",
    "The datasets is aimed at building two classifiers. \n",
    "1. detecting condemnation in a tweet\n",
    "2. estimating the severity of condemnation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "# TODO: Add BibTeX citation\n",
    "# Find for instance the citation on arxiv or on the dataset repo/website\n",
    "_CITATION = \"\"\"\\\n",
    "@InProceedings{huggingface:dataset,\n",
    "title = {A great new dataset},\n",
    "author={huggingface, Inc.\n",
    "},\n",
    "year={2020}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Add description of the dataset here\n",
    "# You can copy an official description\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "The datasets is aimed at building two classifiers. \n",
    "1. detecting condemnation in a tweet\n",
    "2. estimating the severity of condemnation\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Add a link to an official homepage for the dataset here\n",
    "_HOMEPAGE = \"\"\n",
    "\n",
    "# TODO: Add the licence for the dataset here if you can find it\n",
    "_LICENSE = \"\"\n",
    "\n",
    "# TODO: Add link to the official dataset URLs here\n",
    "# The HuggingFace dataset library don't host the datasets but only point to the original files\n",
    "# This can be an arbitrary nested dict/list of URLs (see below in `_split_generators` method)\n",
    "_URLs = {\n",
    "    'first_domain': \"../joes_transgression_ambiguity_project/data/annotated_tweets/ta_tweets.jsonl\",\n",
    "    'second_domain': \"https://huggingface.co/great-new-dataset-second_domain.zip\",\n",
    "}\n",
    "\n",
    "\n",
    "# TODO: Name of the dataset usually match the script name with CamelCase instead of snake_case\n",
    "class CondemnationDataset(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"TODO: Short description of my dataset.\"\"\"\n",
    "\n",
    "    VERSION = datasets.Version(\"1.1.0\")\n",
    "\n",
    "    # This is an example of a dataset with multiple configurations.\n",
    "    # If you don't want/need to define several sub-sets in your dataset,\n",
    "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
    "\n",
    "    # If you need to make complex sub-parts in the datasets with configurable options\n",
    "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\n",
    "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\n",
    "\n",
    "    # You will be able to load one or the other configurations in the following list with\n",
    "    # data = datasets.load_dataset('my_dataset', 'first_domain')\n",
    "    # data = datasets.load_dataset('my_dataset', 'second_domain')\n",
    "    BUILDER_CONFIGS = [\n",
    "        datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"),\n",
    "        datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),\n",
    "    ]\n",
    "\n",
    "    DEFAULT_CONFIG_NAME = \"first_domain\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\n",
    "\n",
    "    def _info(self):\n",
    "        # TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset\n",
    "        if self.config.name == \"first_domain\":  # This is the name of the configuration selected in BUILDER_CONFIGS above\n",
    "            #'text', 'meta', '_input_hash', '_task_hash', 'label', 'score', 'priority', 'spans', '_session_id', '_view_id', 'answer'\n",
    "            features = datasets.Features(\n",
    "                {\n",
    "                    \"text\": datasets.Value(\"string\"),\n",
    "                    \"meta\": datasets.Value(\"dict\"),\n",
    "                    \"_input_hash\": datasets.Value(\"numpy.int64\"),\n",
    "                    \"_task_hash\": datasets.Value(\"numpy.int64\"),\n",
    "                    \"label\": datasets.Value(\"string\"),\n",
    "                    \"score\": datasets.Value(\"numpy.float64\"),\n",
    "                    \"priority\": datasets.Value(\"numpy.float64\"),\n",
    "                    \"spans\": datasets.Value(\"list\"),\n",
    "                    \"_session_id\": dataset.Value(\"string\"),\n",
    "                    \"_view_id\": dataset.Value(\"string\"),\n",
    "                    \"answer\": dataset.Value(\"string\")\n",
    "                    # These are the features of your dataset like images, labels ...\n",
    "                }\n",
    "            )\n",
    "        else:  # This is an example to show how to have different features for \"first_domain\" and \"second_domain\"\n",
    "            features = datasets.Features(\n",
    "                {\n",
    "                    \"sentence\": datasets.Value(\"string\"),\n",
    "                    \"option2\": datasets.Value(\"string\"),\n",
    "                    \"second_domain_answer\": datasets.Value(\"string\")\n",
    "                    # These are the features of your dataset like images, labels ...\n",
    "                }\n",
    "            )\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=_DESCRIPTION,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,  # Here we define them above because they are different between the two configurations\n",
    "            # If there's a common (input, target) tuple from the features,\n",
    "            # specify them here. They'll be used if as_supervised=True in\n",
    "            # builder.as_dataset.\n",
    "            supervised_keys=None,\n",
    "            # Homepage of the dataset for documentation\n",
    "            homepage=_HOMEPAGE,\n",
    "            # License for the dataset if available\n",
    "            license=_LICENSE,\n",
    "            # Citation for the dataset\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        \"\"\"Returns SplitGenerators.\"\"\"\n",
    "        # TODO: This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\n",
    "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\n",
    "\n",
    "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLs\n",
    "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\n",
    "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\n",
    "        my_urls = _URLs[self.config.name]\n",
    "        data_dir = dl_manager.download_and_extract(my_urls)\n",
    "        return [\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TRAIN,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, \"train.jsonl\"),\n",
    "                    \"split\": \"train\",\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TEST,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, \"test.jsonl\"),\n",
    "                    \"split\": \"test\"\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.VALIDATION,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, \"dev.jsonl\"),\n",
    "                    \"split\": \"dev\",\n",
    "                },\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def _generate_examples(\n",
    "        self, filepath, split  # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\n",
    "    ):\n",
    "        \"\"\" Yields examples as (key, example) tuples. \"\"\"\n",
    "        # This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
    "        # The `key` is here for legacy reason (tfds) and is not important in itself.\n",
    "\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            for id_, row in enumerate(f):\n",
    "                data = json.loads(row)\n",
    "                if self.config.name == \"first_domain\":\n",
    "                    yield id_, {\n",
    "                        \"sentence\": data[\"sentence\"],\n",
    "                        \"option1\": data[\"option1\"],\n",
    "                        \"answer\": \"\" if split == \"test\" else data[\"answer\"],\n",
    "                    }\n",
    "                else:\n",
    "                    yield id_, {\n",
    "                        \"sentence\": data[\"sentence\"],\n",
    "                        \"option2\": data[\"option2\"],\n",
    "                        \"second_domain_answer\": \"\" if split == \"test\" else data[\"second_domain_answer\"],\n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53cb7bab",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-9df887cb1a05>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-9df887cb1a05>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dataset = load_dataset(\"jsonl\", data_files= ../joes_transgression_ambiguity_project/data/annotated_tweets/ta_tweets.jsonl)\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"jsonl\", data_files= ../joes_transgression_ambiguity_project/data/annotated_tweets/ta_tweets.jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734138b3",
   "metadata": {},
   "source": [
    "### Testing the dataset loading script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d96bed9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration first_domain-3bcd13f68545b6d6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset condemnation_dataset/first_domain (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/geev/.cache/huggingface/datasets/condemnation_dataset/first_domain-3bcd13f68545b6d6/1.1.0/55c854543a2a937c953122244779cfbe14d55e6ba76866f27d3a9307b474eeee...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Cannot find data file. \nOriginal error:\n[Errno 20] Not a directory: '../joes_transgression_ambiguity_project/data/annotated_tweets/ta_tweets.jsonl/train.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/Research/in_progress/Transgressions/transgressions-env/lib/python3.6/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/in_progress/Transgressions/transgressions-env/lib/python3.6/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                     \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m                 ):\n",
      "\u001b[0;32m~/Research/in_progress/Transgressions/transgressions-env/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/in_progress/Transgressions/transgressions-env/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/dataset_loading_script/55c854543a2a937c953122244779cfbe14d55e6ba76866f27d3a9307b474eeee/dataset_loading_script.py\u001b[0m in \u001b[0;36m_generate_examples\u001b[0;34m(self, filepath, split)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '../joes_transgression_ambiguity_project/data/annotated_tweets/ta_tweets.jsonl/train.jsonl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-81b07f2aea79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./dataset_loading_script.py\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first_domain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../joes_transgression_ambiguity_project/data/annotated_tweets/ta_tweets.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Research/in_progress/Transgressions/transgressions-env/lib/python3.6/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_verifications\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m     )\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/in_progress/Transgressions/transgressions-env/lib/python3.6/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdownloaded_from_gcs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                         self._download_and_prepare(\n\u001b[0;32m--> 609\u001b[0;31m                             \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m                         )\n\u001b[1;32m    611\u001b[0m                     \u001b[0;31m# Sync info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/in_progress/Transgressions/transgressions-env/lib/python3.6/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m                     \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_download_instructions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nOriginal error:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                     \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot find data file. \nOriginal error:\n[Errno 20] Not a directory: '../joes_transgression_ambiguity_project/data/annotated_tweets/ta_tweets.jsonl/train.jsonl'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"./dataset_loading_script.py\", name=\"first_domain\", data_files=\"../joes_transgression_ambiguity_project/data/annotated_tweets/ta_tweets.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c2a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transgression-env",
   "language": "python",
   "name": "transgression-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
