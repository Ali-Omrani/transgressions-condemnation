{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "396b5370",
   "metadata": {},
   "source": [
    "# Condemnation Classifier\n",
    "\n",
    "This notebook holds the code for\n",
    "\n",
    "- loading the condemnation dataset using our dataset_loading_script\n",
    "- preprocessing the dataset which only includes encoding at this stage\n",
    "- running a 10 fold cv on the dataset\n",
    "\n",
    "**Note no hyperparams were tuned!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70c1c2",
   "metadata": {},
   "source": [
    "### Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "488943b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52baee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "# checkpoint = \"roberta-base\"\n",
    "# checkpoint = \"bert-large-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bbf901",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6077c023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset my_dataset_loading_script (/home/geev/.cache/huggingface/datasets/my_dataset_loading_script/condemnation/1.1.0/ef891eb2986445dbe69883df48d98d7039da591e0c35cb211978c71951a3c83e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8457b546b084557b0c8d359bb70f29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"my_dataset_loading_script\", \"condemnation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6a2fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1344\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 336\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce641bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'As multiple sexual allegations come to light with people like <OTHER TARGET 1>, NPRâ€™s -JOHN DOE-, and Michael Fallon, just to name a few...',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_dataset[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e86af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['no_condemnation', 'condemnation'], id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c09c4",
   "metadata": {},
   "source": [
    "### Preprocessing & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb40462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/geev/.cache/huggingface/datasets/my_dataset_loading_script/condemnation/1.1.0/ef891eb2986445dbe69883df48d98d7039da591e0c35cb211978c71951a3c83e/cache-47313ceb021f21df.arrow\n",
      "Loading cached processed dataset at /home/geev/.cache/huggingface/datasets/my_dataset_loading_script/condemnation/1.1.0/ef891eb2986445dbe69883df48d98d7039da591e0c35cb211978c71951a3c83e/cache-b1e535fe82b66242.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c61e4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trainer, tokenized_dataset):\n",
    "    predictions = trainer.predict(tokenized_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "    def get_metrics(y_pred, y_true):\n",
    "        metrics ={}\n",
    "        metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "        metrics[\"precision\"] = precision_score(y_true, y_pred)\n",
    "        metrics[\"recall\"] = recall_score(y_true, y_pred)\n",
    "        metrics[\"f1\"] = f1_score(y_true, y_pred)\n",
    "        return metrics\n",
    "    return get_metrics(preds, predictions.label_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c79c9",
   "metadata": {},
   "source": [
    "### Training with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47260224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "n_split = 10\n",
    "sss = StratifiedShuffleSplit(n_splits=n_split, test_size=1/n_split, random_state=0)\n",
    "sss.get_n_splits(train_dataset, train_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7beb23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9529e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/geev/Research/Transgression/tenv/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1209\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 228\n",
      "/home/geev/Research/Transgression/tenv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [228/228 01:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 336\n",
      "  Batch size = 8\n",
      "/home/geev/Research/Transgression/tenv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./models/hf/hf_fold_{}_model.\n",
      "Configuration saved in ./models/hf/hf_fold_{}_model./config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving a model! for fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./models/hf/hf_fold_{}_model./pytorch_model.bin\n",
      "tokenizer config file saved in ./models/hf/hf_fold_{}_model./tokenizer_config.json\n",
      "Special tokens file saved in ./models/hf/hf_fold_{}_model./special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "performance = {\"test\":{}, \"val\":{}}\n",
    "fold = 1\n",
    "for train_index, val_index in sss.split(train_dataset, train_dataset[\"label\"]):\n",
    "    cv_train_dataset = Dataset.from_dict(train_dataset[train_index])\n",
    "    \n",
    "    training_args = TrainingArguments(\"test_trainer\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset = cv_train_dataset,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    cv_val_dataset = Dataset.from_dict(train_dataset[val_index])\n",
    "\n",
    "    performance[\"val\"][\"fold \"+str(fold)] = evaluate(trainer, cv_val_dataset)\n",
    "    performance[\"test\"][\"fold \"+str(fold)] = evaluate(trainer, test_dataset)\n",
    "\n",
    "    print(\"saving a model! for fold {}\".format(fold))\n",
    "    trainer.save_model(\"./models/hf/hf_fold_{}_model.\")\n",
    "    torch.save(model, \"./models/fold_{}_model.p\".format(fold))\n",
    "    fold+=1\n",
    "    del model\n",
    "    del trainer\n",
    "    del cv_train_dataset\n",
    "    del cv_val_dataset\n",
    "    break\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f14ecf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': {'fold 1': {'accuracy': 0.7738095238095238,\n",
       "   'precision': 0.7848101265822784,\n",
       "   'recall': 0.8815165876777251,\n",
       "   'f1': 0.8303571428571428}},\n",
       " 'val': {'fold 1': {'accuracy': 0.8148148148148148,\n",
       "   'precision': 0.8061224489795918,\n",
       "   'recall': 0.9294117647058824,\n",
       "   'f1': 0.8633879781420767}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de060b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_dump_file = \"10_cv_\"+checkpoint+\"_performance.p\"\n",
    "pickle.dump(performance, open(pickle_dump_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa211c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': {'fold 1': {'accuracy': 0.8184523809523809,\n",
       "   'precision': 0.8537735849056604,\n",
       "   'recall': 0.8578199052132701,\n",
       "   'f1': 0.8557919621749408},\n",
       "  'fold 2': {'accuracy': 0.7678571428571429,\n",
       "   'precision': 0.7878787878787878,\n",
       "   'recall': 0.8625592417061612,\n",
       "   'f1': 0.823529411764706},\n",
       "  'fold 3': {'accuracy': 0.8065476190476191,\n",
       "   'precision': 0.811965811965812,\n",
       "   'recall': 0.9004739336492891,\n",
       "   'f1': 0.8539325842696629},\n",
       "  'fold 4': {'accuracy': 0.7678571428571429,\n",
       "   'precision': 0.821256038647343,\n",
       "   'recall': 0.8056872037914692,\n",
       "   'f1': 0.8133971291866028},\n",
       "  'fold 5': {'accuracy': 0.7857142857142857,\n",
       "   'precision': 0.8293838862559242,\n",
       "   'recall': 0.8293838862559242,\n",
       "   'f1': 0.8293838862559242},\n",
       "  'fold 6': {'accuracy': 0.7797619047619048,\n",
       "   'precision': 0.8215962441314554,\n",
       "   'recall': 0.8293838862559242,\n",
       "   'f1': 0.8254716981132075},\n",
       "  'fold 7': {'accuracy': 0.7678571428571429,\n",
       "   'precision': 0.7878787878787878,\n",
       "   'recall': 0.8625592417061612,\n",
       "   'f1': 0.823529411764706},\n",
       "  'fold 8': {'accuracy': 0.7976190476190477,\n",
       "   'precision': 0.8235294117647058,\n",
       "   'recall': 0.8625592417061612,\n",
       "   'f1': 0.8425925925925926},\n",
       "  'fold 9': {'accuracy': 0.7976190476190477,\n",
       "   'precision': 0.8629441624365483,\n",
       "   'recall': 0.8056872037914692,\n",
       "   'f1': 0.8333333333333333},\n",
       "  'fold 10': {'accuracy': 0.7678571428571429,\n",
       "   'precision': 0.824390243902439,\n",
       "   'recall': 0.8009478672985783,\n",
       "   'f1': 0.8125}},\n",
       " 'val': {'fold 1': {'accuracy': 0.7703703703703704,\n",
       "   'precision': 0.7934782608695652,\n",
       "   'recall': 0.8588235294117647,\n",
       "   'f1': 0.8248587570621468},\n",
       "  'fold 2': {'accuracy': 0.7703703703703704,\n",
       "   'precision': 0.78125,\n",
       "   'recall': 0.8823529411764706,\n",
       "   'f1': 0.8287292817679558},\n",
       "  'fold 3': {'accuracy': 0.7555555555555555,\n",
       "   'precision': 0.7708333333333334,\n",
       "   'recall': 0.8705882352941177,\n",
       "   'f1': 0.8176795580110497},\n",
       "  'fold 4': {'accuracy': 0.6962962962962963,\n",
       "   'precision': 0.75,\n",
       "   'recall': 0.7764705882352941,\n",
       "   'f1': 0.7630057803468209},\n",
       "  'fold 5': {'accuracy': 0.7777777777777778,\n",
       "   'precision': 0.7956989247311828,\n",
       "   'recall': 0.8705882352941177,\n",
       "   'f1': 0.8314606741573034},\n",
       "  'fold 6': {'accuracy': 0.7185185185185186,\n",
       "   'precision': 0.7422680412371134,\n",
       "   'recall': 0.8470588235294118,\n",
       "   'f1': 0.7912087912087912},\n",
       "  'fold 7': {'accuracy': 0.7185185185185186,\n",
       "   'precision': 0.7326732673267327,\n",
       "   'recall': 0.8705882352941177,\n",
       "   'f1': 0.7956989247311828},\n",
       "  'fold 8': {'accuracy': 0.7925925925925926,\n",
       "   'precision': 0.8202247191011236,\n",
       "   'recall': 0.8588235294117647,\n",
       "   'f1': 0.8390804597701149},\n",
       "  'fold 9': {'accuracy': 0.7111111111111111,\n",
       "   'precision': 0.7875,\n",
       "   'recall': 0.7411764705882353,\n",
       "   'f1': 0.7636363636363637},\n",
       "  'fold 10': {'accuracy': 0.762962962962963,\n",
       "   'precision': 0.7912087912087912,\n",
       "   'recall': 0.8470588235294118,\n",
       "   'f1': 0.8181818181818181}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open(pickle_dump_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_performance(performance):\n",
    "    \n",
    "    agg_performance = {}\n",
    "    for key in performance:\n",
    "        agg_performance[key] = {}\n",
    "        for metric in performance[key][\"fold 1\"]:\n",
    "            metric_val_list = [performance[key][fold][metric] for fold in performance[key]]\n",
    "            agg_performance[key][\"avg_\"+metric] = \"{:.2f}\".format(np.mean(metric_val_list))\n",
    "            agg_performance[key][\"std_\"+metric] = \"{:.2f}\".format(np.std(metric_val_list))\n",
    "    return agg_performance\n",
    "        \n",
    "import pprint\n",
    "pprint.pprint(agg_performance(performance)        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e1458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenv",
   "language": "python",
   "name": "tenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
