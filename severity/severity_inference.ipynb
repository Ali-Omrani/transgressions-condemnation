{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ef7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datasets import Value\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import IPython\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ded8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db_metoo_tweets = client[\"jason_twitter\"]\n",
    "metoo_tweets = db_metoo_tweets.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae7fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_cursor = metoo_tweets.find({\"condemnation_prediction\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adfcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "model_path = \"./models/hf\"\n",
    "\n",
    "# model = torch.load(model_path)\n",
    "# model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761ebfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(cursor, n):\n",
    "        result = []\n",
    "        for tweet in tqdm(cursor):\n",
    "            result.append(tweet)\n",
    "            if len(result)==n:\n",
    "                result_to_return = result\n",
    "                result = []\n",
    "                yield result_to_return\n",
    "        yield result\n",
    "        \n",
    "def update_tweet_in_db( document):\n",
    "    try:\n",
    "        metoo_tweets.update_one(\n",
    "            {'_id': document['_id']},\n",
    "            {'$set': document}\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\"couldn't update \", document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7e3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"exp/bart/results\",\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=128,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "# training_args = TrainingArguments(\"test-trainer\")\n",
    "# training_args.eval_accumulation_steps = 1  # pushes predictions out of GPU to mitigate GPU out of memory\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"models/hf/hf_fold_1_model\"),\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d32bee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "102it [00:00, 184.07it/s]\u001b[A\n",
      "3547it [00:01, 4008.09it/s]\u001b[A\n",
      "6638it [00:01, 4840.61it/s]\u001b[A\n",
      "9732it [00:01, 6598.38it/s]\u001b[A\n",
      "12829it [00:02, 5849.06it/s]\u001b[A\n",
      "15939it [00:03, 5065.47it/s]\u001b[A\n",
      "19167it [00:03, 6477.35it/s]\u001b[A\n",
      "22427it [00:04, 5255.63it/s]\u001b[A\n",
      "25701it [00:04, 6602.52it/s]\u001b[A\n",
      "28961it [00:04, 7641.65it/s]\u001b[A\n",
      "32151it [00:05, 5234.58it/s]\u001b[A\n",
      "35338it [00:06, 6238.40it/s]\u001b[A\n",
      "38617it [00:06, 7290.98it/s]\u001b[A\n",
      "41889it [00:07, 4520.23it/s]\u001b[A\n",
      "45149it [00:08, 5576.97it/s]\u001b[A\n",
      "48319it [00:08, 6683.60it/s]\u001b[A\n",
      "51401it [00:08, 7761.89it/s]\u001b[A\n",
      "54609it [00:10, 4351.20it/s]\u001b[A\n",
      "57887it [00:10, 5577.87it/s]\u001b[A\n",
      "61166it [00:10, 6895.77it/s]\u001b[A\n",
      "64517it [00:10, 8327.66it/s]\u001b[A\n",
      "67957it [00:10, 9675.02it/s]\u001b[A\n",
      "71353it [00:12, 4244.48it/s]\u001b[A\n",
      "74757it [00:12, 5432.61it/s]\u001b[A\n",
      "78113it [00:13, 6769.50it/s]\u001b[A\n",
      "81474it [00:13, 8139.28it/s]\u001b[A\n",
      "84890it [00:13, 9593.68it/s]\u001b[A\n",
      "88266it [00:13, 10766.77it/s]\u001b[A\n",
      "91656it [00:16, 3707.29it/s] \u001b[A\n",
      "95027it [00:16, 4800.72it/s]\u001b[A\n",
      "98387it [00:16, 6035.22it/s]\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2402f3e9624f4ec789e53d54ce0803df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 02:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [03:11, ?it/s]35.22it/s]\u001b[A\n",
      "99999it [03:11, 523.14it/s] \n"
     ]
    }
   ],
   "source": [
    "n=100000\n",
    "target_col = \"clean_tweet\"\n",
    "for idx, chunk in tqdm(enumerate(split_list(tweet_cursor, n))):\n",
    "        df = pd.DataFrame(chunk)\n",
    "        if not len(df):\n",
    "            break\n",
    "        pred_df = df[[target_col]].dropna()\n",
    "        pred_data = df.dropna(subset=[target_col])\n",
    "        pred_dataset = Dataset.from_pandas(pred_df)\n",
    "        pred_dataset = pred_dataset.rename_column(target_col, \"text\")\n",
    "        tokenized_datasets = pred_dataset.map(tokenize_function, batched=True)\n",
    "        predictions_logits = trainer.predict(tokenized_datasets)\n",
    "        preds = np.argmax(predictions_logits.predictions, axis = 1)\n",
    "        save_df = pred_data[[\"_id\", \"clean_tweet\"]]\n",
    "        save_df[\"severity_prediction\"] = preds\n",
    "        save_df[\"severity_logit_0\"] = predictions_logits.predictions[:, 0]\n",
    "        save_df[\"severity_logit_1\"] = predictions_logits.predictions[:, 1]\n",
    "        save_df[\"severity_logit_2\"] = predictions_logits.predictions[:, 1]\n",
    "    \n",
    "        del tokenized_datasets\n",
    "        del pred_dataset\n",
    "        for idx, row in tqdm(save_df.iterrows()):\n",
    "            update_tweet_in_db(row.to_dict())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ec3c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geev/Research/Transgression/tenv/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/geev/Research/Transgression/tenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/geev/Research/Transgression/tenv/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/geev/Research/Transgression/tenv/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "save_df = pred_data[[\"_id\", \"clean_tweet\"]]\n",
    "save_df[\"severity_prediction\"] = preds\n",
    "save_df[\"severity_logit_0\"] = predictions_logits.predictions[:, 0]\n",
    "save_df[\"severity_logit_1\"] = predictions_logits.predictions[:, 1]\n",
    "save_df[\"severity_logit_2\"] = predictions_logits.predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43228cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>severity_prediction</th>\n",
       "      <th>severity_logit_0</th>\n",
       "      <th>severity_logit_1</th>\n",
       "      <th>severity_logit_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6383e2adfa2b796ff3841c3f</td>\n",
       "      <td>&lt;TWEET&gt;: The latest #Predator movie raised a l...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.418743</td>\n",
       "      <td>0.922649</td>\n",
       "      <td>0.922649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6383e2adfa2b796ff3841c46</td>\n",
       "      <td>&lt;TWEET&gt;: Don't cast your child molesting pedop...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.805397</td>\n",
       "      <td>0.236971</td>\n",
       "      <td>0.236971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6383e2adfa2b796ff3841c4b</td>\n",
       "      <td>&lt;TWEET&gt;: I just saw #ThePredator’s promoted tw...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.350736</td>\n",
       "      <td>0.753730</td>\n",
       "      <td>0.753730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6383e2adfa2b796ff3841c4c</td>\n",
       "      <td>&lt;TWEET&gt;: Perhaps if Steven Wilder Striegel had...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.378258</td>\n",
       "      <td>0.814329</td>\n",
       "      <td>0.814329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6383e2adfa2b796ff3841c4e</td>\n",
       "      <td>&lt;TWEET&gt;: The former teen known as Jane Doe, no...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.725376</td>\n",
       "      <td>1.061773</td>\n",
       "      <td>1.061773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>6383e319fa2b796ff38a1f27</td>\n",
       "      <td>&lt;TWEET&gt;: Wait, so Kevin Spacey's response to b...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.849389</td>\n",
       "      <td>0.973997</td>\n",
       "      <td>0.973997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>6383e319fa2b796ff38a1f29</td>\n",
       "      <td>&lt;TWEET&gt;: I see Kevin Spacey has come out as a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.355826</td>\n",
       "      <td>0.965941</td>\n",
       "      <td>0.965941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>6383e319fa2b796ff38a1f2e</td>\n",
       "      <td>&lt;TWEET&gt;: Why does Kevin Spacey have to be ruin...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.044263</td>\n",
       "      <td>0.708245</td>\n",
       "      <td>0.708245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>6383e319fa2b796ff38a1f31</td>\n",
       "      <td>&lt;TWEET&gt;: Bingo!(this is the 1 that came forwar...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.326478</td>\n",
       "      <td>1.006863</td>\n",
       "      <td>1.006863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>6383e319fa2b796ff38a1f3d</td>\n",
       "      <td>&lt;TWEET&gt;: Uhh someone needs to tell Kevin Space...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.568369</td>\n",
       "      <td>0.979408</td>\n",
       "      <td>0.979408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _id  \\\n",
       "0      6383e2adfa2b796ff3841c3f   \n",
       "1      6383e2adfa2b796ff3841c46   \n",
       "2      6383e2adfa2b796ff3841c4b   \n",
       "3      6383e2adfa2b796ff3841c4c   \n",
       "4      6383e2adfa2b796ff3841c4e   \n",
       "...                         ...   \n",
       "99995  6383e319fa2b796ff38a1f27   \n",
       "99996  6383e319fa2b796ff38a1f29   \n",
       "99997  6383e319fa2b796ff38a1f2e   \n",
       "99998  6383e319fa2b796ff38a1f31   \n",
       "99999  6383e319fa2b796ff38a1f3d   \n",
       "\n",
       "                                             clean_tweet  severity_prediction  \\\n",
       "0      <TWEET>: The latest #Predator movie raised a l...                    1   \n",
       "1      <TWEET>: Don't cast your child molesting pedop...                    2   \n",
       "2      <TWEET>: I just saw #ThePredator’s promoted tw...                    1   \n",
       "3      <TWEET>: Perhaps if Steven Wilder Striegel had...                    1   \n",
       "4      <TWEET>: The former teen known as Jane Doe, no...                    1   \n",
       "...                                                  ...                  ...   \n",
       "99995  <TWEET>: Wait, so Kevin Spacey's response to b...                    1   \n",
       "99996  <TWEET>: I see Kevin Spacey has come out as a ...                    1   \n",
       "99997  <TWEET>: Why does Kevin Spacey have to be ruin...                    1   \n",
       "99998  <TWEET>: Bingo!(this is the 1 that came forwar...                    1   \n",
       "99999  <TWEET>: Uhh someone needs to tell Kevin Space...                    1   \n",
       "\n",
       "       severity_logit_0  severity_logit_1  severity_logit_2  \n",
       "0             -1.418743          0.922649          0.922649  \n",
       "1             -1.805397          0.236971          0.236971  \n",
       "2             -1.350736          0.753730          0.753730  \n",
       "3             -0.378258          0.814329          0.814329  \n",
       "4             -0.725376          1.061773          1.061773  \n",
       "...                 ...               ...               ...  \n",
       "99995         -0.849389          0.973997          0.973997  \n",
       "99996         -1.355826          0.965941          0.965941  \n",
       "99997         -0.044263          0.708245          0.708245  \n",
       "99998         -1.326478          1.006863          1.006863  \n",
       "99999         -0.568369          0.979408          0.979408  \n",
       "\n",
       "[100000 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c46c8d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
       "       1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1,\n",
       "       1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1,\n",
       "       1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2,\n",
       "       1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2,\n",
       "       2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2,\n",
       "       1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 0, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2,\n",
       "       2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1,\n",
       "       2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1,\n",
       "       2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1,\n",
       "       1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2,\n",
       "       1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2,\n",
       "       2, 1, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c4e3d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-1.4187431 ,  0.9226494 , -0.09052195],\n",
       "       [-1.8053972 ,  0.23697093,  1.5364958 ],\n",
       "       [-1.3507361 ,  0.7537301 ,  0.0857865 ],\n",
       "       ...,\n",
       "       [-0.04426257,  0.7082449 , -0.8602782 ],\n",
       "       [-1.3264781 ,  1.0068629 ,  0.06774779],\n",
       "       [-0.56836873,  0.97940767, -0.70287985]], dtype=float32), label_ids=None, metrics={'test_runtime': 169.983, 'test_samples_per_second': 588.294, 'test_steps_per_second': 2.3})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894afd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet_masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;TWEET&gt;: » “I do love you. . I love you, &lt;TARG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;TWEET&gt;: &lt;TARGET 1&gt; apologizes for 'aggressive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;TWEET&gt;: No one should have to endure this kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;TWEET&gt;: \"New &lt;TARGET 1&gt; sexual assault accusa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;TWEET&gt;: Yes this. &lt;TARGET 1&gt;, this clown, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683897</th>\n",
       "      <td>&lt;TWEET&gt;: On the one year anniversary of the Ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683898</th>\n",
       "      <td>&lt;TWEET&gt;: &lt;TARGET 1&gt;'s photobombing. The ladies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683900</th>\n",
       "      <td>&lt;TWEET&gt;: But but but dude you're a rapist HOW ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683901</th>\n",
       "      <td>&lt;TWEET&gt;: Lisa Bloom, Lawyer Advising &lt;TARGET 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683902</th>\n",
       "      <td>&lt;TWEET&gt;: Don Jr. trolls silent Hillary over &lt;T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2797605 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_tweet_masked\n",
       "0        <TWEET>: » “I do love you. . I love you, <TARG...\n",
       "4        <TWEET>: <TARGET 1> apologizes for 'aggressive...\n",
       "5        <TWEET>: No one should have to endure this kin...\n",
       "6        <TWEET>: \"New <TARGET 1> sexual assault accusa...\n",
       "8        <TWEET>: Yes this. <TARGET 1>, this clown, the...\n",
       "...                                                    ...\n",
       "4683897  <TWEET>: On the one year anniversary of the Ac...\n",
       "4683898  <TWEET>: <TARGET 1>'s photobombing. The ladies...\n",
       "4683900  <TWEET>: But but but dude you're a rapist HOW ...\n",
       "4683901  <TWEET>: Lisa Bloom, Lawyer Advising <TARGET 1...\n",
       "4683902  <TWEET>: Don Jr. trolls silent Hillary over <T...\n",
       "\n",
       "[2797605 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('../../data/5_mil_7days_metoo.p', 'rb')\n",
    "pred_data = pickle.load(file)\n",
    "file.close()\n",
    "pred_df = pred_data[[\"clean_tweet_masked\"]].dropna()\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f424aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdae47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Value\n",
    "pred_dataset = Dataset.from_pandas(pred_df)\n",
    "pred_dataset = pred_dataset.rename_column(\"clean_tweet_masked\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e91bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073e949ea3f44947b85a5d7924cd7efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "tokenized_datasets = pred_dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd3f7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./sample_model.p\"\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "# model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d468910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"exp/bart/results\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "training_args.eval_accumulation_steps=1 #pushes predictions out of GPU to mitigate GPU out of memory\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dcdbc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./severity_predictions', 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenv",
   "language": "python",
   "name": "tenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
